# config
# It is Coming from GLP-common library and in Future it can be used : Crosscutting
spring.security.enable-csrf=false
feature.oauth2.enabled=false
feature.security.enabled=false

# Automatically create views and indexes by "@ViewIndexed", "@N1qlPrimaryIndexed","@N1qlSecondaryIndexed".
# It will create the index in event of NO index for the concerned bucket : Devops
spring.data.couchbase.auto-index=true

# Couchbase  userYour Username and password of couchbase
database=COUCHBASESERVER
couchbaseserver.bucketName=${COUCHBASE_SERVER_BUCKETNAME}
couchbaseserver.bucketPassword=
couchbaseserver.hostNames=${COUCHBASE_SERVER_HOSTNAME}
couchbaseserver.cluster.username=${CLUSTER_USERNAME}
couchbaseserver.cluster.password=${CLUSTER_PASSWORD}

glp.core.data.couchbase.enabled=true
glp.core.data.couchbase.bucket=${COUCHBASE_SERVER_BUCKETNAME}
glp.core.data.couchbase.bucketPassword=
glp.core.data.couchbase.hosts=${COUCHBASE_SERVER_HOSTNAME}
glp.core.data.couchbase.username=${CLUSTER_USERNAME}
glp.core.data.couchbase.password=${CLUSTER_PASSWORD}

#WhiteListed URLs

whitelisted.URL=${WHITELISTED_URL}

server.port=${SERVER_PORT}

# logging
# logging
logging.level.root=${ROOT_LOG_LEVEL}
logging.level.org.springframework.web=${SPRING_LOG_LEVEL}
logging.level.com.pearson.glp.crosscutting.isc.saga.service=${ROOT_LOG_LEVEL}
logging.level.com.pearson.glp.lap=${COM_PEARSON_LOG_LEVEL}
logging.level.request_trace=${REQUEST_TRACE_LOG_LEVEL}
logging.file-path=${LOG_FILE_PATH}

# configuration of distributed tracing
spring.zipkin.service.name=lap-{ENV_BASE_URL}
spring.zipkin.sender.type=web
spring.zipkin.baseUrl=http://zipkin:9411/
spring.sleuth.sampler.probability=1.0
spring.sleuth.web.client.enabled=true

### ISC Configuration ###
isc.saga.timeout.fixedDelay=30000

#### Common Kafka Properties ###
#bootstrap.servers=${ISC_KAFKA_PORT}
spring.cloud.stream.kafka.binder.brokers=${ISC_KAFKA_PORT}

### ConsumerConfiguration.properties ###
spring.kafka.consumer.group-id=glpv2lap
enable.auto.commit=true
auto.commit.interval.ms=1000
session.timeout.ms=30000
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
poll.interval=1000

### ProducerConfiguration.properties ###
acks=all
retries=0
batch.size=10
linger.ms=1
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer

### ZookeeperConfiguration.properties ###
zookeeper.hosts=${ISC_ZOOKEEPER_PORT}
session.time.out.in.ms=15000
connection.time.out.in.ms=10000
no.of.partitions=${ISC_ZOOKEEPER_PARTITIONS}
no.of.replication=${ISC_ZOOKEEPER_REPLICATIONS}


###Validation Properties###
config.home=/opt/apps/config
lpb.base.url=http://lpb:9080/lpb

#pagination properties
# This is going to change in future so it is in NOTSEE list : Devops
default.page.number=1
default.page.size=100
max.page.size=100

glp.core.validation.enabled=true
glp.core.validation.rest-context.enabled=true
glp.core.validation.fail-fast.enabled=false

#kafka publish events
event.publish.enabled=${PUBLISH_EVENT}

### Cache ###
cache.max.limit=${CACHE_MAX_LIMIT}
cache.service.enabled=${CACHE_SERVICE_ENABLED}
cache.service.duration=${CACHE_SERVICE_DURATION}
